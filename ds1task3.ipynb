{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformasi dan Rekayasa Fitur untuk Kedua Dataset \n",
    "- Mengubah variabel kategorikal menjadi format numerik (misalnya, one-hot \n",
    "encoding, ordinal encoding, atau frequency encoding) untuk masing-masing \n",
    "dataset. \n",
    "- Melakukan normalisasi atau standarisasi fitur numerik jika diperlukan. \n",
    "- Menciptakan minimal satu fitur baru yang dapat meningkatkan kualitas dataset. \n",
    "- Menyusun laporan tentang dampak transformasi fitur terhadap dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_acc = pd.read_csv('accepted_cleaned.csv')\n",
    "# df_rej = pd.read_csv('rejected_cleaned.csv')\n",
    "df_tes = pd.read_csv('application_test_cleaned.csv')\n",
    "df_tra = pd.read_csv('application_train_cleaned.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([column for column in df_acc.columns if df_acc[column].dtype == object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc.term.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_values = {' 36 months': 36, ' 60 months': 60}\n",
    "df_acc['term'] = df_acc.term.map(term_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc.term.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc.drop(['id','member_id','grade','desc','url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = [column for column in df_acc.columns if df_acc[column].dtype == object]\n",
    "df_acc = pd.get_dummies(df_acc, columns=dummies, drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below for the rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([column for column in df_rej.columns if df_rej[column].dtype == object])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rej[''].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in df_tra:\n",
    "    if df_tra[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(df_tra[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(df_tra[col])\n",
    "            # Transform both training and testing data\n",
    "            df_tra[col] = le.transform(df_tra[col])\n",
    "            df_tes[col] = le.transform(df_tes[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of categorical variables\n",
    "df_tra = pd.get_dummies(df_tra)\n",
    "df_tes = pd.get_dummies(df_tes)\n",
    "\n",
    "print('Training Features shape: ', df_tra.shape)\n",
    "print('Testing Features shape: ', df_tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_labels = df_tes['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "df_tra, df_tes = df_tra.align(df_tes, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "df_tra['TARGET'] = tra_labels\n",
    "\n",
    "print('Training Features shape: ', df_tra.shape)\n",
    "print('Testing Features shape: ', df_tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in df_tra:\n",
    "    if isinstance(df_tra[dataset_name], pd.Series):  \n",
    "        df_tra[dataset_name] = df_tra[dataset_name].to_frame()  # Ubah Series ke DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "for col in df_tra.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    if df_tra[col].skew().mean() > 1:  # Jika distribusi skewed → Gunakan MinMaxScaler\n",
    "        df_tra[col] = minmax_scaler.fit_transform(df_tra[[col]])\n",
    "    else:  # Jika distribusi normal → Gunakan StandardScaler\n",
    "        df_tra[col] = standard_scaler.fit_transform(df_tra[[col]])\n",
    "\n",
    "for col in df_tes.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    if df_tes[col].skew().mean() > 1:  # Jika distribusi skewed → Gunakan MinMaxScaler\n",
    "        df_tes[col] = minmax_scaler.fit_transform(df_tes[[col]])\n",
    "    else:  # Jika distribusi normal → Gunakan StandardScaler\n",
    "        df_tes[col] = standard_scaler.fit_transform(df_tes[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tes.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tes.to_csv(\"app_test_cleaned_encoded.csv\", index=False)\n",
    "df_tra.to_csv(\"app_train_cleaned_encoded.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
